{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Scrapping a webpage using BeautifulSoup\n",
    "\n",
    "[`Beautiful Soup`](https://www.crummy.com/software/BeautifulSoup/) is a Python library to search and extract what we need from a document. I use it to access the data in `Geno 2.0 Next Generation` [webpage](https://genographic.nationalgeographic.com/reference-populations-next-gen/) for each population. The overall workflow is as follows:\n",
    "1. Identify a source, whether a website url or locally saved file.\n",
    "2. Use a parser to parse HTML codes of the source. Default is `html.parser` but for run this notebook, you need to install `html5lib` library to parse sources written in HTML5. To install see [here](https://pypi.python.org/pypi/html5lib).\n",
    "3. Find HTML elements  such as `div` or `a` that hold the required information. We can also select elements with certain `id` or `class`. \n",
    "4. Then use commands such as `findAll` or `find` to find all or an instance of the information, you are looking for.\n",
    "5. Possibly do a post-process on the found information, to make it in the required format. Here, I collect them in an ordered dictionary to convert the dataset in JSON at the end.\n",
    "\n",
    "This script uses three libraries:\n",
    "* `BeautifulSoup`: To scrape the webpage\n",
    "* `Collections`: To hold an ordered list of items in a dictionary\n",
    "* `json`: To save extracted data in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the source\n",
    "\n",
    "I have saved a local copy of the webpage in `webpage` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# url to scrape\n",
    "url_to_scrape = 'https://genographic.nationalgeographic.com/reference-populations-next-gen/'\n",
    "# local file to scrape\n",
    "file_to_scrape = open(\"./webpage/Reference Populations - Geno 2.0 Next Generation.html\")\n",
    "# Create a beautifulsoup object from html content\n",
    "soup = BeautifulSoup(file_to_scrape,\"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking into soup!\n",
    "Let's see what is inside the variable `soup`. It contains all HTML elements in the webpage. Looking through the code, I realized the info that I'm interested in are wrapped in `<div>` elements that look like this:  \n",
    "\n",
    "```\n",
    "<div class=\"pop-211\">\n",
    "...\n",
    "</div>\n",
    "```\n",
    "The class name is `pop-x` where `x` ranges from 200 to 260. But we don't need to know the exact range if we use `except` command in Python. Within each of these `<div>` elements, there are a few `<li>` items which look like the following block:\n",
    "\n",
    "\n",
    "```\n",
    "<li class=\"pop-id-2105\" style=\"width:8%;\">\n",
    "            <div class=\"wp-autosomal-bar-label\">\n",
    "                <p>Eastern Africa</p>\n",
    "                <div class=\"wp-autosomal-bar-line\"></div>\n",
    "            </div>\n",
    "            <div class=\"wp-autosomal-bar-section\">\n",
    "                <h3>2%</h3>\n",
    "            </div>\n",
    "        </li>\n",
    "```\n",
    "We are interested in the strings within `<p>` (`<p>Eastern Africa</p>`) and `<h3>` (`<h3>2%</h3>`) tags. So the idea is this:\n",
    "\n",
    "1. Find all `<div>` elements with `class=pop-x`,\n",
    "2. Extract the text within `<p>` elements in `<div class=\"wp-autosomal-bar-label\">`.\n",
    "3. Extract the text within `<h3>` elements in `<div class=\"wp-autosomal-bar-section\">`.\n",
    "\n",
    "Number two gives the population name and number three gives us the percentage. We're ready to implement the algorithm.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create an empty parent dictionary containing \n",
    "# dictionaries for all labels\n",
    "dic = []\n",
    "\n",
    "for identifier in range(200,270):\n",
    "    # make sure you use a wide enough range\n",
    "    # to include all possible numbers\n",
    "    \n",
    "    # create an ordered dictionary to keep \n",
    "    # all info about genetic contributions\n",
    "    # of this identifier\n",
    "    d = OrderedDict()\n",
    "    \n",
    "    try:\n",
    "        # find all `div elements corresponding to `identifier`\n",
    "        # This contains all HTML codes within that <div> \n",
    "        data = soup.findAll('div', class_=\"pop-\"+str(identifier))[0]\n",
    "        \n",
    "        # Population selected to find its genetic contributions\n",
    "        population_label = cells.findAll('h3')[0].get_text()\n",
    "        d['title'] = population_label\n",
    "        \n",
    "        # How much each gene contributes in the selected populations\n",
    "        # find <div>s with the mentioned classes \n",
    "        label = [key.find('p').text for key in data.findAll('div',class_=\"wp-autosomal-bar-label\")]\n",
    "        percent = [key.find('h3').text for key in data.findAll('div',class_=\"wp-autosomal-bar-section\")]\n",
    "        \n",
    "        # make sure that we have the number of labels\n",
    "        # and percentages match!\n",
    "        if (len(label)==len(percent)):\n",
    "            # if yes, put them in an ordered dictionary\n",
    "            for i in range(len(label)):\n",
    "                d[label[i]]=percent[i].split('%')[0]\n",
    "        \n",
    "        # append the ordered dictionary to the parent dictionary\n",
    "        dic.append(d)\n",
    "    \n",
    "    except:\n",
    "        # if identifier does not exist, do not return an IndexError\n",
    "        IndexError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could see how a dictionary for each label looks like. It contains a `title` with a set of `labels` and `values` for each genetic type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('title', 'Ashkenazi Jewish'),\n",
       "             ('Eastern Europe', '2'),\n",
       "             ('Finland & Northern Siberia', '28'),\n",
       "             ('Eastern Asia', '18'),\n",
       "             ('Central Asia', '42'),\n",
       "             ('Asia Minor', '8'),\n",
       "             ('Southern Asia', '2')])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Saving the results\n",
    "Finally we can save all the results in a `JSON` (JavaScript Object Notation) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(dic, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
