{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Scrapping Geno 2.0 Next Generation in Python using BeautifulSoup\n",
    "\n",
    "[`Beautiful Soup`](https://www.crummy.com/software/BeautifulSoup/) is a Python library to search and extract what we need from a document. In this post, I use it to access the data in `Geno 2.0 Next Generation` [webpage](https://genographic.nationalgeographic.com/reference-populations-next-gen/) for each population. This project contains information of more than 830000 volunteers from 140 countries who have participated in the project. The webpage summarizes the results and shows what is share of various genetic affiliations in each population. Possibly, novel insights is hidden in the data, but first of all we need to have them in a place!\n",
    "\n",
    "The overall workflow looks like this:\n",
    "1. Identify a source, whether a website url or a locally saved file.\n",
    "2. In BeautifulSoup, use a parser to parse HTML source code. The default is `html.parser`. Other options include `html5lib` library to parse sources written in HTML5, but you have install it separately. see the instructions [here](https://pypi.python.org/pypi/html5lib).\n",
    "3. Find HTML elements  such as `div` or `a` that hold the required data. We can also select elements with certain `id` or `class`. \n",
    "4. Then use commands such as `findAll` or `find` to find all or an instance of the data, you are looking for.\n",
    "5. Possibly do a post-process on the scraped data, to make it in the required format. Here, I collect them in an ordered dictionary to convert the dataset into `JSON` file and `pandas` dataframe.\n",
    "\n",
    "This script uses libraries:\n",
    "* `BeautifulSoup`: To scrape the webpage\n",
    "* `Collections`: To hold an ordered list of items in a dictionary\n",
    "* `json`: To save extracted data in JSON format\n",
    "* `pandas`: To create a dataframe\n",
    "* `numpy`: To do numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the source\n",
    "\n",
    "As I said in the above workflow, we could use either a locally save file or a url. The most usual way is to use a url using libraries like `urllib` or `requests`. But for this particular webpage that I am intrested in, we are not able to extract all data because part of data is generated through javascript code. There are workarounds to access data in webpages rendered by javascript (like [`dryscrape`](https://github.com/niklasb/dryscrape)), but since I only work with 1 page, it is easier to save it locally. I have saved a local copy of the webpage in `webpage` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url to scrape\n",
    "url_to_scrape = 'https://genographic.nationalgeographic.com/reference-populations-next-gen/'\n",
    "\n",
    "# local file to scrape\n",
    "file_to_scrape = open(\"./webpage/Reference Populations - Geno 2.0 Next Generation.html\")\n",
    "# Create a beautifulsoup object from html content\n",
    "soup = BeautifulSoup(file_to_scrape,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking into soup!\n",
    "Let's see what is inside the variable `soup`. It contains all HTML elements in the webpage. Looking through the code, I realized the info that I'm interested in are wrapped in `<div>` elements that look like this:  \n",
    "\n",
    "```\n",
    "<div class=\"pop-211\">\n",
    "...\n",
    "</div>\n",
    "```\n",
    "The class name is `pop-x` where `x` ranges from 200 to 260. But we don't need to know the exact range, as we will see later. Within each of these `<div>` elements, there are a few `<li>` items which look like the following block:\n",
    "\n",
    "\n",
    "```\n",
    "<li class=\"pop-id-2105\" style=\"width:8%;\">\n",
    "            <div class=\"wp-autosomal-bar-label\">\n",
    "                <p>Eastern Africa</p>\n",
    "                <div class=\"wp-autosomal-bar-line\"></div>\n",
    "            </div>\n",
    "            <div class=\"wp-autosomal-bar-section\">\n",
    "                <h3>2%</h3>\n",
    "            </div>\n",
    "        </li>\n",
    "```\n",
    "We are interested in the strings within `<p>` (`<p>Eastern Africa</p>`) and `<h3>` (`<h3>2%</h3>`) tags. So the idea is this:\n",
    "\n",
    "1. Find all `<div>` elements with `class=pop-x`,\n",
    "2. Extract the text within `<p>` elements in `<div class=\"wp-autosomal-bar-label\">`.\n",
    "3. Extract the text within `<h3>` elements in `<div class=\"wp-autosomal-bar-section\">`.\n",
    "\n",
    "Number two gives the population name and number three gives us the percentage. We're ready to crawl the webpage and extract the data. The idea is going through the page and collect the data in a dictionary. Later, we use the dictionaries to create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create an empty parent dictionary containing \n",
    "# dictionaries for all labels\n",
    "dic = []\n",
    "\n",
    "for identifier in range(200,261):\n",
    "    # make sure you use a wide enough range\n",
    "    # to include all possible numbers\n",
    "    \n",
    "    # create an ordered dictionary to keep \n",
    "    # all info about genetic contributions\n",
    "    # of this identifier\n",
    "    d = OrderedDict()\n",
    "    \n",
    "    # find all `div elements corresponding to `identifier`\n",
    "    # This contains all HTML codes within that <div> \n",
    "    data = soup.findAll('div', class_=\"pop-\"+str(identifier))[0]\n",
    "\n",
    "    # Population selected to find its genetic contributions\n",
    "    population_label = data.findAll('h3')[0].get_text()\n",
    "    d['title'] = population_label\n",
    "\n",
    "    # How much each gene contributes in the selected populations\n",
    "    # find <div>s with the mentioned classes \n",
    "    label = [key.find('p').text for key in data.findAll('div',class_=\"wp-autosomal-bar-label\")]\n",
    "    percent = [key.find('h3').text for key in data.findAll('div',class_=\"wp-autosomal-bar-section\")]\n",
    "\n",
    "    # make sure that the number of labels\n",
    "    # and percentages match!\n",
    "    if (len(label)==len(percent)):\n",
    "        # if yes, put them in an ordered dictionary\n",
    "        for i in range(len(label)):\n",
    "            d[label[i]]=percent[i].split('%')[0]\n",
    "\n",
    "    # append the ordered dictionary to the parent dictionary\n",
    "    dic.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could see how a dictionary for each label looks like. It contains a `title` (e.g. Chinese) with a set of `labels` (e.g. 'Finland & Northern Siberia',...) and `values` (e.g. '2', ...) for each genetic type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('title', 'Chinese'),\n",
       "             ('Finland & Northern Siberia', '2'),\n",
       "             ('Eastern Asia', '81'),\n",
       "             ('Central Asia', '8'),\n",
       "             ('Southeast Asia & Oceania', '7')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Saving the results\n",
    "Finally we need to decide what is the best way to store data in a file. For example, I can save all the results in a `JSON` (JavaScript Object Notation) file to use it later in a `D3` visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('data.json', 'w') as outfile:\\n    json.dump(dic, outfile)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with open('data.json', 'w') as outfile:\n",
    "    json.dump(dic, outfile)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a very common way is to save data in a dataframe. We need to achieve a dataframe that looks like this:\n",
    "\n",
    "| Population | Arabia |\tAsia Minor | \n",
    "|---|---|---|\n",
    "| African-American (Southwestern US) | 0 | 0 | \n",
    "| Altaian (Siberian) | 0 | 0 | \n",
    "| Amerindian (Mexico) | 0 | 0 | \n",
    "\n",
    "So all populations are stored in a column while each regional affiliation has its own column. The numbers show percentage of the share of regional affiliations in that population. So I need to find out all regional affiliations plus all populations by looping through `dic` which hold all scraped data. Since an affiliations can appears in more than one population, we need to find unique affiliations, so we use `set` class to hold them. We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all regional affiliations and sort them\n",
    "regions = sorted(list(set([keys for v in dic for keys in v if keys!='title'])))\n",
    "# find all populations\n",
    "titles = [v['title'] for v in dic]\n",
    "# what is the number of rows in our dataset?\n",
    "n = len(titles)\n",
    "# initialize a dataset but set the share equal to zero temporarily\n",
    "columns=OrderedDict()\n",
    "for r in regions:\n",
    "    print\n",
    "    columns[r]=np.zeros(n)\n",
    "df = pd.DataFrame(columns,index=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we created the dataframe using pandas but the all elements are zero. So we loop through `dic` again to fill the dataset with scraped values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabia</th>\n",
       "      <th>Asia Minor</th>\n",
       "      <th>Central Asia</th>\n",
       "      <th>Eastern Africa</th>\n",
       "      <th>Eastern Asia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African-American (Southwestern US)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altaian (Siberian)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amerindian (Mexico)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bermudian</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bougainville-Nasioi (Oceania)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Arabia  Asia Minor  Central Asia  \\\n",
       "African-American (Southwestern US)     0.0         0.0           0.0   \n",
       "Altaian (Siberian)                     0.0         8.0          42.0   \n",
       "Amerindian (Mexico)                    0.0         0.0           0.0   \n",
       "Bermudian                              0.0         0.0           0.0   \n",
       "Bougainville-Nasioi (Oceania)          0.0         0.0           0.0   \n",
       "\n",
       "                                    Eastern Africa  Eastern Asia  \n",
       "African-American (Southwestern US)             2.0           0.0  \n",
       "Altaian (Siberian)                             0.0          18.0  \n",
       "Amerindian (Mexico)                            0.0           0.0  \n",
       "Bermudian                                      0.0           0.0  \n",
       "Bougainville-Nasioi (Oceania)                  0.0          13.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in dic:\n",
    "    # the population\n",
    "    title = d['title']\n",
    "    # select the row related the title\n",
    "    row = df.loc[title]\n",
    "    # fill the cell using the percentage value\n",
    "    for k in d:\n",
    "        if k!='title': row[k]=d[k]\n",
    "df.ix[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset contains the percentage values and if an affiliation does not contribute into a population, its share is 0. Finally I convert the populations into a column on their own and save the dataset to use it in my next project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Arabia</th>\n",
       "      <th>Asia Minor</th>\n",
       "      <th>Central Asia</th>\n",
       "      <th>Eastern Africa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>African-American (Southwestern US)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Altaian (Siberian)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amerindian (Mexico)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bermudian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bougainville-Nasioi (Oceania)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>British (United Kingdom)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                index  Arabia  Asia Minor  Central Asia  \\\n",
       "0  African-American (Southwestern US)     0.0         0.0           0.0   \n",
       "1                  Altaian (Siberian)     0.0         8.0          42.0   \n",
       "2                 Amerindian (Mexico)     0.0         0.0           0.0   \n",
       "3                           Bermudian     0.0         0.0           0.0   \n",
       "4       Bougainville-Nasioi (Oceania)     0.0         0.0           0.0   \n",
       "5            British (United Kingdom)     0.0         0.0           0.0   \n",
       "\n",
       "   Eastern Africa  \n",
       "0             2.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "5             0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.ix[0:5,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./geno2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is written in Jupyter notebook and is available with the required dataset as a [github repository](https://github.com/Mahdisadjadi/gene-similarity)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
